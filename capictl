#!/bin/bash
set -o errexit -o pipefail

function usage() {
    echo "Usage: $0 [flags]"
    echo
    echo "    -n <name>               - The name of the cluster (required)"
    echo "    -v <version>            - The Kubernetes version to use (default to latest version)"
    echo "    -b <bootstrap-provider> - Valid values for the bootstrap cluster provider are 'kind', 'k3d' (defaults to 'kind')"
    echo "    -i <workload-provider>  - Valid values for the infrastructure provider are 'hetzner', 'vsphere', 'docker' (defaults to 'docker')"
    echo "    -p <cni-plugin>         - Valid values for the CNI plugin are 'cilium' (defaults to 'cilium')"
    echo "    -x <cidr>               - The pod CIDR to use (defaults to 10.42.18.0/17)"
    echo "    -y <cidr>               - The service CIDR to use (defaults to 10.42.0.0/17)"
    echo "    -c <count>              - The number of control plane nodes (defaults to 1)"
    echo "    -w <count>              - The number of worker nodes (defaults to 2)"
    exit 1
}

while getopts "n:v:b:i:p:c:w:" o; do
    case "${o}" in
        n)
            CLUSTER_NAME="${OPTARG}"
            ;;
        v)
            KUBERNETES_VERSION="${OPTARG}"
            ;;
        b)
            BOOTSTRAP_CLUSTER_PROVIDER="${OPTARG}"
            ;;
        i)
            INFRASTRUCTURE_PROVIDER="${OPTARG}"
            ;;
        p)
            CNI_PLUGIN="${OPTARG}"
            ;;
        x)
            POD_CIDR="${OPTARG}"
            ;;
        y)
            SERVICE_CIDR="${OPTARG}"
            ;;
        c)
            CONTROL_PLANE_NODE_COUNT="${OPTARG}"
            ;;
        w)
            WORKER_NODE_COUNT="${OPTARG}"
            ;;
        *)
            echo "ERROR: Unknown option ${o}"
            usage
            ;;
    esac
done
shift $((OPTIND-1))

if test -z "${CLUSTER_NAME}"; then
    usage
fi

if test -f .env; then
    source .env
fi

: "${TIMEOUT_IN_MINUTES:=30}"

: "${BOOTSTRAP_CLUSTER_PROVIDER:=kind}"
: "${CNI_PLUGIN:=cilium}"
: "${CONTROL_PLANE_NODE_COUNT:=1}"
: "${WORKER_NODE_COUNT:=2}"
: "${INFRASTRUCTURE_PROVIDER:=docker}"

: "${BOOTSTRAP_PROVIDER:=kubeadm}"
: "${CONTROL_PLANE_PROVIDER:=kubeadm}"

: "${REUSE_CLUSTER_YAML:=false}"

if ! type clusterctl >/dev/null 2>&1; then
    echo "ERROR: clusterctl not found"
    exit 1
fi
if ! type kubectl >/dev/null 2>&1; then
    echo "ERROR: kubectl not found"
    exit 1
fi
if ! type jq >/dev/null 2>&1; then
    echo "ERROR: jq not found"
    exit 1
fi

if test -z "${KUBERNETES_VERSION}"; then
    KUBERNETES_VERSION="$(
        kubectl version --client --output json \
        | jq --raw-output ".clientVersion.gitVersion" \
        | tr -d v
    )"
fi

BOOTSTRAP_CLUSTER_NAME="capi-bootstrap-${CLUSTER_NAME}"
if ! test -f "bootstrap/${BOOTSTRAP_CLUSTER_PROVIDER}.sh"; then
    echo "ERROR: Bootstrap provider ${BOOTSTRAP_CLUSTER_PROVIDER} not found"
    exit 1
fi
if ! test -f "provider/${INFRASTRUCTURE_PROVIDER}.sh"; then
    echo "ERROR: Workload provider ${INFRASTRUCTURE_PROVIDER} not found"
    exit 1
fi
if ! test -f "cni/${CNI_PLUGIN}.sh"; then
    echo "ERROR: CNI provider ${CNI_PLUGIN} not found"
    exit 1
fi
source functions.sh
source "bootstrap/${BOOTSTRAP_CLUSTER_PROVIDER}.sh"
source "provider/${INFRASTRUCTURE_PROVIDER}.sh"
source "cni/${CNI_PLUGIN}.sh"

if ! bootstrap_precheck; then
    echo "ERROR: Bootstrap provider precheck failed"
    exit 1
fi
if ! workload_precheck; then
    echo "ERROR: Workload provider precheck failed"
    exit 1
fi

function github_get_rate_limit() {
    if test -n "${GITHUB_TOKEN}"; then
        GITHUB_TOKEN_AUTH="Authorization: token ${GITHUB_TOKEN}"
    fi
    curl \
        --silent \
        --location \
        --fail \
        --header "${GITHUB_TOKEN_AUTH}" \
        --url https://api.github.com/rate_limit \
    | jq --raw-output '.rate.remaining'
}

GH_RATE_LIMIT_LEFT="$(github_get_rate_limit)"
if test "${GH_RATE_LIMIT_LEFT}" -lt 10; then
    echo "ERROR: GitHub rate limit too low (${GH_RATE_LIMIT_LEFT} left)"
    exit 1
fi

bootstrap_create "${BOOTSTRAP_CLUSTER_NAME}"
touch kubeconfig-bootstrap
chmod 0600 kubeconfig-bootstrap
bootstrap_kubeconfig "${BOOTSTRAP_CLUSTER_NAME}"
if ! test -f "kubeconfig-bootstrap"; then
    echo "ERROR: kubeconfig-bootstrap not found"
    bootstrap_delete "${BOOTSTRAP_CLUSTER_NAME}"
    exit 1
fi
export KUBECONFIG="kubeconfig-bootstrap"
if ! bootstrap_post_create_hook; then
    echo "ERROR: Failed to execute post create hook."
    bootstrap_delete "${BOOTSTRAP_CLUSTER_NAME}"
    exit 1
fi
if ! kubectl cluster-info; then
    echo "ERROR: Failed to connect to bootstrap cluster"
    bootstrap_delete "${BOOTSTRAP_CLUSTER_NAME}"
    exit 1
fi

echo "### Installing provider"
clusterctl init \
    --infrastructure "${INFRASTRUCTURE_PROVIDER}" \
    --bootstrap "${BOOTSTRAP_PROVIDER}" \
    --control-plane "${CONTROL_PLANE_PROVIDER}" \
    --wait-providers \
    --wait-provider-timeout 300
if ! bootstrap_post_init_hook; then
    echo "ERROR: Failed to execute post init hook"
    bootstrap_delete "${BOOTSTRAP_CLUSTER_NAME}"
    exit 1
fi
while kubectl get pods -A | tail -n +2 | grep -vqE "(Running|Completed)"; do
    echo "### Waiting for all pods to be running..."
    sleep 10
done

echo "### Generating cluster configuration"
if test -n "${PROVIDER_TEMPLATE_FLAVOR}"; then
    PROVIDER_TEMPLATE_FLAVOR_PARAMETER="--flavor ${PROVIDER_TEMPLATE_FLAVOR}"
fi
if ! ${REUSE_CLUSTER_YAML}; then
    rm -f cluster.yaml
fi
if test -z HCLOUD_SSH_KEY; then
    SSH_KEY_NAME=caph
else
    SSH_KEY_NAME="${HCLOUD_SSH_KEY}"
fi
export SSH_KEY_NAME
if ! test -f cluster.yaml; then
    clusterctl generate cluster "${CLUSTER_NAME}" \
        --kubernetes-version="v${KUBERNETES_VERSION}" \
        ${PROVIDER_TEMPLATE_FLAVOR_PARAMETER} \
        --control-plane-machine-count="${CONTROL_PLANE_NODE_COUNT}" \
        --worker-machine-count="${WORKER_NODE_COUNT}" \
        >cluster.yaml
fi
if test -n "${IMAGE_NAME}"; then
    sed -i -E "s/^(\s+imageName:) .+$/\1 ${IMAGE_NAME}/" cluster.yaml
else
    echo "ERROR: IMAGE_NAME not set"
    exit 1
fi
if test -n "${POD_CIDR}"; then
    export POD_CIDR
    yq --inplace eval 'select(.kind == "Cluster").spec.clusterNetwork.pods.cidrBlocks |= [env(POD_CIDR)]' cluster.yaml
fi
if test -n "${SERVICE_CIDR}"; then
    export SERVICE_CIDR
    yq --inplace eval 'select(.kind == "Cluster").spec.clusterNetwork.services.cidrBlocks |= [env(SERVICE_CIDR)]' cluster.yaml
fi
workload_post_generate_hook "${CLUSTER_NAME}"

echo "### Rolling out workload cluster"
workload_pre_apply_hook "${CLUSTER_NAME}"
cat cluster.yaml \
| kubectl apply -f -
workload_post_apply_hook "${CLUSTER_NAME}"

if ! kubectl wait cluster ${CLUSTER_NAME} --for condition=ControlPlaneInitialized --timeout=${TIMEOUT_IN_MINUTES}m; then
    echo "ERROR: Control plane failed to initialize"
    clusterctl describe cluster ${CLUSTER_NAME} --show-conditions all
    exit 1
fi

echo "### Getting kubeconfig for workload cluster ${CLUSTER_NAME}"
touch "kubeconfig-${CLUSTER_NAME}"
chmod 0600 "kubeconfig-${CLUSTER_NAME}"
clusterctl get kubeconfig ${CLUSTER_NAME} >"kubeconfig-${CLUSTER_NAME}"

workload_control_plane_initialized_hook "${CLUSTER_NAME}"

echo "### Deploy CNI plugin"
if ! cni_precheck; then
    echo "ERROR: CNI provider precheck failed"
    exit 1
fi
cni_deploy "${CLUSTER_NAME}"

if ! kubectl wait  cluster ${CLUSTER_NAME} --for condition=Ready --timeout=${TIMEOUT_IN_MINUTES}m; then
    echo "ERROR: Control plane failed to become ready"
    clusterctl describe cluster ${CLUSTER_NAME} --show-conditions all
    exit 1
fi
echo "### Controle plane ready"
if ! kubectl wait machines --selector cluster.x-k8s.io/control-plane-name=${CLUSTER_NAME}-control-plane --for condition=NodeHealthy --timeout=${TIMEOUT_IN_MINUTES}m; then
    echo "ERROR: Control plane machines failed to become healthy"
    clusterctl describe cluster ${CLUSTER_NAME} --show-conditions all
    exit 1
fi
echo "### Control plane machines healthy"

if ! kubectl wait machinedeployment ${CLUSTER_NAME}-md-0 --for condition=Ready  --timeout=${TIMEOUT_IN_MINUTES}m; then
    echo "ERROR: Nodes failed to become ready"
    clusterctl describe cluster ${CLUSTER_NAME} --show-conditions all
    kubectl describe machinedeployment ${CLUSTER_NAME}-md-0
    exit 1
fi
if ! kubectl --kubeconfig=kubeconfig-${CLUSTER_NAME} wait nodes --all --all-namespaces --for condition=Ready --timeout=30m; then
    echo "### Nodes are not ready"
    kubectl get nodes --all-namespaces
    exit 1
fi
echo "### Worker nodes are ready"

if ! kubectl --kubeconfig=kubeconfig-${CLUSTER_NAME} wait pods --all --all-namespaces --for condition=Ready --timeout=${TIMEOUT_IN_MINUTES}m; then
    echo "ERROR: Pods failed to become ready"
    kubectl get pods --all-namespaces
    exit 1
fi
echo "### Pods are ready"

if ! cni_post_install_hook "${CLUSTER_NAME}"; then
    echo "ERROR: CNI provider post install hook failed"
    exit 1
fi

echo "### Fetching logs"
kubectl --namespace capi-system logs deployment/capi-controller-manager \
>capi-controller-manager.log
kubectl --namespace capi-kubeadm-bootstrap-system logs deployment/capi-kubeadm-bootstrap-controller-manager \
>capi-kubeadm-bootstrap-controller-manager.log
kubectl --namespace capi-kubeadm-control-plane-system logs deployment/capi-kubeadm-control-plane-controller-manager \
>capi-kubeadm-control-plane-controller-manager.log
if ! workload_logs "${CLUSTER_NAME}"; then
    echo "ERROR: Failed to fetch logs for workload provider"
    exit 1
fi

echo "### Initialize CAPH in workload cluster"
clusterctl init --bootstrap "${CLUSTERCTL_INIT_BOOTSTRAP}" --control-plane "${CLUSTERCTL_INIT_CONTROL_PLANE}" --kubeconfig kubeconfig-${CLUSTER_NAME} --infrastructure hetzner --wait-providers

echo "### Waiting for management resources to be running"
if ! kubectl --kubeconfig=kubeconfig-${CLUSTER_NAME} wait pods --all --all-namespaces --for condition=Ready --timeout=30m; then
    echo "### Pods are not ready"
    kubectl get pods --all-namespaces
    exit 1
fi
echo "### Pods are ready"
echo "### Move management resources to workload cluster"
clusterctl move --to-kubeconfig kubeconfig-${CLUSTER_NAME}

echo "### Creating cluster admin"
mv kubeconfig-${CLUSTER_NAME} kubeconfig-${CLUSTER_NAME}-certificate
export KUBECONFIG=kubeconfig-${CLUSTER_NAME}-certificate
cat <<EOF | kubectl --namespace kube-system apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-cluster-admin
  namespace: kube-system
EOF
cat <<EOF | kubectl --namespace kube-system apply -f -
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-cluster-admin
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: my-cluster-admin
  namespace: kube-system
EOF
cat <<EOF | kubectl --namespace kube-system apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: my-cluster-admin-token
  annotations:
    kubernetes.io/service-account.name: my-cluster-admin
type: kubernetes.io/service-account-token
EOF
TOKEN=$(
    kubectl --namespace kube-system get secrets my-cluster-admin-token -o json \
    | jq --raw-output '.data.token' \
    | base64 -d
)
SERVER=$(
    kubectl config view --raw --output json \
    | jq --raw-output '.clusters[].cluster.server'
)
CA=$(
    kubectl config view --raw --output json \
    | jq --raw-output '.clusters[].cluster."certificate-authority-data"' \
    | base64 -d
)
export KUBECONFIG=kubeconfig-${CLUSTER_NAME}
if test -f "${KUBECONFIG}"; then
    echo "kubeconfig ${KUBECONFIG} already exists" >&2
    exit 1
fi
touch "${KUBECONFIG}"
kubectl config set-cluster default --server="${SERVER}" --certificate-authority=<(echo "${CA}") --embed-certs=true
kubectl config set-credentials my-cluster-admin --token="${TOKEN}"
kubectl config set-context cluster-admin --cluster=default --user=my-cluster-admin
kubectl config use-context cluster-admin

echo "### Removing bootstrap cluster"
bootstrap_delete "${CLUSTER_NAME}"
